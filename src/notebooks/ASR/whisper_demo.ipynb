{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Whisper Demo\n",
    "\n",
    "modified from:\n",
    "[Whisper Github](https://github.com/openai/whisper#python-usage)\n",
    "\n",
    "Use MP3 file extracted from brief \"laughter lift\" video here: \n",
    "[Wedding Receptions, North London Pubs and French Toast - Laughter Lift](https://www.youtube.com/watch?v=1R29NlHOoGA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from dotenv import find_dotenv\n",
    "# NOTE: empty `.env` file was added beneath `src` directory. Ignored by gitignore rules.\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(find_dotenv()))\n",
    "from notebooks.notebook_utils import DevData\n",
    "\n",
    "# ----------------------------------------\n",
    "import jiwer\n",
    "import whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mp3_file exists: True\n"
     ]
    }
   ],
   "source": [
    "# define paths\n",
    "external_dir = DevData().external_dir\n",
    "mp3_file = os.path.join(external_dir, \"Laughter_Lift.mp3\")\n",
    "print(f\"mp3_file exists: {os.path.exists(mp3_file)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo the `base` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_model = whisper.load_model(\"base\")\n",
    "base_result = bs_model.transcribe(mp3_file, fp16=False)\n",
    "print(base_result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo the `small` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_model = whisper.load_model(\"small\")\n",
    "sm_result = sm_model.transcribe(mp3_file, fp16=False)\n",
    "print(sm_result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo the `medium` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_model = whisper.load_model(\"medium\")\n",
    "md_result = md_model.transcribe(mp3_file, fp16=False)\n",
    "print(md_result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of results\n",
    "\n",
    "- Use `jiwer` package to compute WER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = \"More with Emily Watson in take two, ads in a minute. But first, it's time once again, very, very good news everybody, we step into our laughter lift. Huzzah. Shazam. Here we go. Hey Mark, seeing as you loved the noble gases joke so much last week, I was going to tell you another one, but all the good ones are gone. I got that, I got that, because argon's a gas. It is, it's a noble gas. I got it, okay. Anyway, here's another sciencey one for you. are you ready? I was out at a pub with rooms in showbiz North London on Saturday for a wedding reception. A neutron walked in. How much for a pint, he said. For you, no charge, said the barman. And I get that as well, because a neutron has no charge. Subatomic particle with no charge. And then a photon walked in. I'd like a room for the night, please, he said. Certainly, do you have any luggage we can take up to the room? Asked the receptionist. No, said the photon. I'm traveling light. Because a photon is a... It is a light particle. Quantum of light. It was in fact... It's an education today. It's not funny, but it's an education. It was in fact cousin Cecil's wedding to his delightful Parisian fiancee Noémie on Saturday. At the reception, I raised my champagne glass and said, A dish of sliced bread soaked in beaten eggs and often milk or cream, then pan fried. Alternative names and variants include eggy bread, Bombay toast, gypsy toast, and poor nights of Windsor. It was a French toast. The evening did not end well. I got the bar bill and had a massive row with the bar staff. I argued with my cashier that the bill was £70.20, not £7,000... £7,020. He didn't get the point. Anyway, what have we got? You've got that as well. Yes. Yes, got that. What's still to come? Dungeons and Dragons. Okay, back after this. Unless you're a vanguardista, in which case you definitely don't have a nickname that everyone else knows apart from you and your service will not be interrupted.Thanks very much for watching this video I hope you enjoyed watching it. While you're here, check out all the other videos because they're cool too, aren't they? Yeah, and if you want to keep up to date with everything Kermode and Mayo's take, then check out our social channels. I mean, why wouldn't you? I mean, I would. I have done. Excellent.\"\n",
    "\n",
    "base_wer = wer(correct, base_result[\"text\"])\n",
    "sm_wer = wer(correct, sm_result[\"text\"])\n",
    "md_wer = wer(correct, md_result[\"text\"])\n",
    "print(f\"base WER:\\t{base_wer}\")\n",
    "print(f\"small WER:\\t{sm_wer}\")\n",
    "print(f\"medium WER:\\t{md_wer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Findings\n",
    "\n",
    "`base` model is not satisfactory. The punctuation and sentences are problematic and a number of words are wrong.\n",
    "`medium` model is much much better. It seems to have correct sentences/punctuation. Most of the problems in `base` are resolved. It even got the French name Noémie correct. One thing `medium` got wrong that `base` got right was \"showbiz north London\" as opposed to \"Chobhub north London\". Another thing `medium` got wrong was \"Vanguard Easter\" instead of \"vanguardista\". But this isn't a common term so is forgivable. I think both of these examples (and other Wittertainment jargon) could be improved with model fine-tuning.\n",
    "\n",
    "### Running time\n",
    "| model  | time   |\n",
    "| -----  | ------ |\n",
    "| base   | 0m 32s |\n",
    "| medium | 4m 56s |\n",
    "\n",
    "### Guessing at full processing time\n",
    "Note: the youtube video the audio was extracted from is only 2:08 minutes. The full Take 1 podcast is generally a bit over an hour. If the Whisper processing time scales linearly this would mean approximately 11 hours for ONE podcast! n.b. the running times here are local using CPU. This suggests the necessity of using GPU processing.\n",
    "\n",
    "### Word Error Rate (WER)\n",
    "The WER metric demonstrates a marked improvement using the `medium` vs `base` model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diarization using WhisperX\n",
    "\n",
    "IMPORTANT\n",
    "Note the Requirements\n",
    "\n",
    "Requirements\n",
    "- Install pyannote.audio 3.0 with pip install pyannote.audio\n",
    "- Accept pyannote/segmentation-3.0 user conditions\n",
    "- Accept pyannote/speaker-diarization-3.0 user conditions\n",
    "- Create access token at hf.co/settings/tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperx import load_align_model, align\n",
    "from whisperx.diarize import DiarizationPipeline, assign_word_speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"HUGGINGFACE_TOKEN\"] = \"\"\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "diarization_pipeline = DiarizationPipeline(use_auth_token=os.environ[\"HUGGINGFACE_TOKEN\"])\n",
    "diarization_result = diarization_pipeline(mp3_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------  take 2 ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dacd2cc40ad4e95b28255d941fdedf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/5.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69513aac95b46da9a8fe71b7b58fa65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.yaml:   0%|          | 0.00/399 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b7a3adf6de4e1080f85f4034dbde41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading speaker-embedding.onnx:   0%|          | 0.00/26.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# instantiate the pipeline\n",
    "from pyannote.audio import Pipeline\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "  \"pyannote/speaker-diarization-3.0\",\n",
    "  use_auth_token=os.environ[\"HUGGINGFACE_TOKEN\"])\n",
    "\n",
    "# run the pipeline on an audio file\n",
    "diarization = pipeline(mp3_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outfile = os.path.join(external_dir, \"audio_out.rttm\")\n",
    "# dump the diarization output to disk using RTTM format\n",
    "with open(outfile, \"w\") as rttm:\n",
    "   diarization.write_rttm(rttm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------  take 3 ----------\n",
    "# putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisperx import load_align_model, align\n",
    "from whisperx.diarize import DiarizationPipeline, assign_word_speakers\n",
    "\n",
    "model = whisper.load_model(\"medium\")\n",
    "result = model.transcribe(mp3_file, fp16=False)\n",
    "\n",
    "language_code = result[\"language\"]\n",
    "segments = result[\"segments\"]\n",
    "\n",
    "\n",
    "device: str = \"cpu\"\n",
    "model_a, metadata = load_align_model(language_code=language_code, device=device)\n",
    "aligned_segments = align(segments, model_a, metadata, mp3_file, device)\n",
    "\n",
    "\n",
    "diarization_pipeline = DiarizationPipeline(use_auth_token=os.environ[\"HUGGINGFACE_TOKEN\"])\n",
    "diarization_result = diarization_pipeline(mp3_file)\n",
    "\n",
    "\n",
    "result_segments, word_seg = assign_word_speakers(diarization_result, segments)\n",
    "\n",
    "results_segments_w_speakers = []\n",
    "for result_segment in result_segments:\n",
    "    results_segments_w_speakers.append(\n",
    "        {\n",
    "            \"start\": result_segment[\"start\"],\n",
    "            \"end\": result_segment[\"end\"],\n",
    "            \"text\": result_segment[\"text\"],\n",
    "            \"speaker\": result_segment[\"speaker\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Print the results in a user-friendly way\n",
    "for i, segment in enumerate(results_segments_w_speakers):\n",
    "    print(f\"Segment {i + 1}:\")\n",
    "    print(f\"Start time: {segment['start']:.2f}\")\n",
    "    print(f\"End time: {segment['end']:.2f}\")\n",
    "    print(f\"Speaker: {segment['speaker']}\")\n",
    "    print(f\"Transcript: {segment['text']}\")\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "witter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
