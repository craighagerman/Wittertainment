# On Speech To Text


## On Libraries / Systems for ASR
see:
[Top 10 Open Source Speech Recognition/Speech-to-Text Systems](https://fosspost.org/open-source-speech-recognition/)


## OpenAI Whisper

[OpenAI Whisper ](https://openai.com/research/whisper)
[Hugging Face Whisper](https://huggingface.co/openai/whisper-medium)
[OpenAI Whisper github](https://github.com/openai/whisper#python-usage)


## Evaluating ASR

[How to evaluate Speech Recognition models](https://www.assemblyai.com/blog/how-to-evaluate-speech-recognition-models/)


## Diarization with OpenAI Whisper

[Speaker Diarization Using OpenAI Whisper](https://github.com/MahmoudAshraf97/whisper-diarization)
[pyannote-audio](https://github.com/pyannote/pyannote-audio)
[whisperx](https://github.com/m-bain/whisperX)


## On Deploying OpenAI Whisper

see:
[Whisper Deployment Decisions: Part I â€” Evaluating Latency, Costs, and Performance Metrics](https://blog.ml6.eu/whisper-deployment-decisions-part-i-evaluating-latency-costs-and-performance-metrics-d07f6edc9ec0)


## Glossary

### Diarisation

> the process of partitioning an audio stream containing human speech into homogeneous segments according to the identity of each speaker. (wikipedia)

- partitioning audio to identify which speaker said what
- Speaker identification

Helpful links:
- Transcription and diarization (speaker identification): [https://github.com/openai/whisper/discussions/264]
- Whisper's transcription plus Pyannote's Diarization: [https://github.com/Majdoddin/nlp]
- Neural speaker diarization with pyannote.audio: [https://github.com/pyannote/pyannote-audio]
- Speaker diarization (partitioning audio based on speaker identity): [https://github.com/openai/whisper/discussions/104]
- Working Google Colab project (must convert audio to Mono): [https://colab.research.google.com/drive/11ccdRYZSHBbUYI9grn6S1O67FVyvCKyL]



